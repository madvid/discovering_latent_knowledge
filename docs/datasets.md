# Detailled on datasets

Here we go in detailled in the processes about datasets used in the paper *Discovering Latent Knowledge* [[1]](#1).


## Datasets used
Burns *et al.* used 10 datasets to perform tests on the caption of model belief of the truthness concept:
1. [imdb](link) dataset,
2. [Amazon policy](link) dataset,
3. [AG-News](link) dataset,
4. [DBpedia-14](link) dataset,
5. [NLI-RTE](link) dataset,
6. [QNLI](link) dataset,
7. [COPA](link) dataset,
8. [Story-Cloze](link) dataset,
9. [BoolQ](link) dataset,
10. [PIQA](link) dataset.

Here some detailled about each datasets:

### Amazon policy dataset
lorem ipsum

### AG-News dataset
lorem ipsum

### DBpedia-14 dataset
lorem ipsum

### NLI dataset - subset RTE
lorem ipsum

### QNLI dataset 
lorem ipsum

### COPA dataset
lorem ipsum

### Story-Cloze dataset
lorem ipsum

### BoolQ dataset
lorem ipsum

### PIQA dataset
lorem ipsum



## Multiple shade of prompt
### PromptSource library
The authors use [promptsource library](https://github.com/bigscience-workshop/promptsource) to generate multiple versions of prompt forged from the examples in the dataset. 



## References
<a id="1">[1]</a> 
Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob, *Discovering Latent Knowledge in Language Models Without Supervision*, ArXiV, **2022**.

<!---
@article{burns2022dl,
  title={Discovering Latent Knowledge in Language Models Without Supervision},
  author={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  journal={ArXiV},
  year={2022}
}
-->